{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Tracking\n",
    "- 실험을 여러개 할때 그 과정과 결과를 좀 더 편리하게 모으고 파악하기 좋게 만들어 주는 법에 대해서 다룬다\n",
    "- ML하려면 Hyperparameter 조정해 가면서 이런저런 시도를 해봐야 할 텐데 그때 꽤 유용할 것으로 보인다\n",
    "\n",
    "### 1. TensorBoard 대충 실행해 보기\n",
    "- 1-1 : 데이터 다운로드\n",
    "- 1-2 : Transform 복습 / DataLoader 만들기\n",
    "- 1-3 : Pretrained model 조정 복습 / summary로 보기 복습\n",
    "- 1-4 : torch.utils.tensorboard.SummaryWriter를 사용해 Track하도록 train loop 변형\n",
    "- 1-5 : 다 갖춰졌으니 training 해보고 TensorBoard로 보기\n",
    "    - VSCode에서 Ctrl + Shift + P -> Python: Launch TensorBoard 로 볼수 있음\n",
    "\n",
    "### 2. 여러개 실행하고 실행한거 트랙해 보기\n",
    "- 2-1 : SummaryWriter 생성 도구 (대략 어느 폴더에 어떻게 잘 눈에 띄게 저장할까)\n",
    "- 2-2 : Data download & Transforms 준비 & DataLoader 준비\n",
    "    - 원래 EfficientB0와 EfficientB2는 서로 다른 transform을 갖고 있는데 책에서는 그냥 같은 transform 적용. 일단 그거 그대로 따라가긴 했는데 진짜로 할때는 그렇게 하면 안되겠지\n",
    "- 2-3 : Model 생성 도구\n",
    "- 2-4 : 여러 다른 조건에서 테스트 후 TensorBoard로 보기\n",
    "    - 근데 막 엄청 차이나지는 않음 역시..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data/pizza_steak_sushi directory exists, skipping download.\n",
      "image_path: data/pizza_steak_sushi\n"
     ]
    }
   ],
   "source": [
    "# 1-1 : 데이터 다운로드\n",
    "\n",
    "from going_modular import *\n",
    "\n",
    "image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
    "                           destination=\"pizza_steak_sushi\")\n",
    "print(f'image_path: {image_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manual_transforms: Compose(\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=warn)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n",
      "automatic_transforms: ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[256]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BICUBIC\n",
      ")\n",
      "len(train_dataloader.dataset): 225\n",
      "len(test_dataloader.dataset): 75\n",
      "class_names: ['pizza', 'steak', 'sushi']\n"
     ]
    }
   ],
   "source": [
    "# 1-2 : Transform 복습 / DataLoader 만들기\n",
    "\n",
    "# Create DataLoaders\n",
    "import torchvision\n",
    "\n",
    "train_dir = image_path / 'train'\n",
    "test_dir = image_path / 'test'\n",
    "\n",
    "# Manual version\n",
    "manual_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize( mean = [0.485, 0.456, 0.406],  \n",
    "                                       std = [0.229, 0.224, 0.225])\n",
    "])\n",
    "print(f'manual_transforms: {manual_transforms}')\n",
    "\n",
    "# Automatic version\n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "automatic_transforms = weights.transforms()\n",
    "print(f'automatic_transforms: {automatic_transforms}')\n",
    "\n",
    "# We use automatic version\n",
    "train_dataloader, test_dataloader, class_names = create_dataloaders(\n",
    "                                                    train_dir=train_dir,\n",
    "                                                    test_dir=test_dir,\n",
    "                                                    transform=automatic_transforms,\n",
    "                                                    batch_size=32\n",
    "                                                )\n",
    "print(f'len(train_dataloader.dataset): {len(train_dataloader.dataset)}')\n",
    "print(f'len(test_dataloader.dataset): {len(test_dataloader.dataset)}')\n",
    "print(f'class_names: {class_names}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape               Output Shape              Param #                   Trainable\n",
       "================================================================================================================================================================\n",
       "EfficientNet (EfficientNet)                                  [32, 3, 224, 224]         [32, 3]                   --                        Partial\n",
       "├─Sequential (features)                                      [32, 3, 224, 224]         [32, 1280, 7, 7]          --                        False\n",
       "│    └─Conv2dNormActivation (0)                              [32, 3, 224, 224]         [32, 32, 112, 112]        --                        False\n",
       "│    │    └─Conv2d (0)                                       [32, 3, 224, 224]         [32, 32, 112, 112]        (864)                     False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 32, 112, 112]        [32, 32, 112, 112]        (64)                      False\n",
       "│    │    └─SiLU (2)                                         [32, 32, 112, 112]        [32, 32, 112, 112]        --                        --\n",
       "│    └─Sequential (1)                                        [32, 32, 112, 112]        [32, 16, 112, 112]        --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 32, 112, 112]        [32, 16, 112, 112]        (1,448)                   False\n",
       "│    └─Sequential (2)                                        [32, 16, 112, 112]        [32, 24, 56, 56]          --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 16, 112, 112]        [32, 24, 56, 56]          (6,004)                   False\n",
       "│    │    └─MBConv (1)                                       [32, 24, 56, 56]          [32, 24, 56, 56]          (10,710)                  False\n",
       "│    └─Sequential (3)                                        [32, 24, 56, 56]          [32, 40, 28, 28]          --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 24, 56, 56]          [32, 40, 28, 28]          (15,350)                  False\n",
       "│    │    └─MBConv (1)                                       [32, 40, 28, 28]          [32, 40, 28, 28]          (31,290)                  False\n",
       "│    └─Sequential (4)                                        [32, 40, 28, 28]          [32, 80, 14, 14]          --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 40, 28, 28]          [32, 80, 14, 14]          (37,130)                  False\n",
       "│    │    └─MBConv (1)                                       [32, 80, 14, 14]          [32, 80, 14, 14]          (102,900)                 False\n",
       "│    │    └─MBConv (2)                                       [32, 80, 14, 14]          [32, 80, 14, 14]          (102,900)                 False\n",
       "│    └─Sequential (5)                                        [32, 80, 14, 14]          [32, 112, 14, 14]         --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 80, 14, 14]          [32, 112, 14, 14]         (126,004)                 False\n",
       "│    │    └─MBConv (1)                                       [32, 112, 14, 14]         [32, 112, 14, 14]         (208,572)                 False\n",
       "│    │    └─MBConv (2)                                       [32, 112, 14, 14]         [32, 112, 14, 14]         (208,572)                 False\n",
       "│    └─Sequential (6)                                        [32, 112, 14, 14]         [32, 192, 7, 7]           --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 112, 14, 14]         [32, 192, 7, 7]           (262,492)                 False\n",
       "│    │    └─MBConv (1)                                       [32, 192, 7, 7]           [32, 192, 7, 7]           (587,952)                 False\n",
       "│    │    └─MBConv (2)                                       [32, 192, 7, 7]           [32, 192, 7, 7]           (587,952)                 False\n",
       "│    │    └─MBConv (3)                                       [32, 192, 7, 7]           [32, 192, 7, 7]           (587,952)                 False\n",
       "│    └─Sequential (7)                                        [32, 192, 7, 7]           [32, 320, 7, 7]           --                        False\n",
       "│    │    └─MBConv (0)                                       [32, 192, 7, 7]           [32, 320, 7, 7]           (717,232)                 False\n",
       "│    └─Conv2dNormActivation (8)                              [32, 320, 7, 7]           [32, 1280, 7, 7]          --                        False\n",
       "│    │    └─Conv2d (0)                                       [32, 320, 7, 7]           [32, 1280, 7, 7]          (409,600)                 False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 1280, 7, 7]          [32, 1280, 7, 7]          (2,560)                   False\n",
       "│    │    └─SiLU (2)                                         [32, 1280, 7, 7]          [32, 1280, 7, 7]          --                        --\n",
       "├─AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]          [32, 1280, 1, 1]          --                        --\n",
       "├─Sequential (classifier)                                    [32, 1280]                [32, 3]                   --                        True\n",
       "│    └─Dropout (0)                                           [32, 1280]                [32, 1280]                --                        --\n",
       "│    └─Linear (1)                                            [32, 1280]                [32, 3]                   3,843                     True\n",
       "================================================================================================================================================================\n",
       "Total params: 4,011,391\n",
       "Trainable params: 3,843\n",
       "Non-trainable params: 4,007,548\n",
       "Total mult-adds (Units.GIGABYTES): 12.31\n",
       "================================================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 3452.09\n",
       "Params size (MB): 16.05\n",
       "Estimated Total Size (MB): 3487.41\n",
       "================================================================================================================================================================"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-3 : Pretrained model 조정 복습 / summary로 보기 복습\n",
    "\n",
    "import torchinfo\n",
    "\n",
    "# Get pretrained model, freeze base layers and change the classifier\n",
    "\n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
    "\n",
    "# Freeze base layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Change the classifier\n",
    "model.classifier = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(0.2, inplace=True),\n",
    "    torch.nn.Linear(in_features=1280,\n",
    "                    out_features=len(class_names),\n",
    "                    bias=True)\n",
    ").to(device)\n",
    "\n",
    "# print summary\n",
    "torchinfo.summary(\n",
    "            model, \n",
    "            input_size=(32, 3, 224, 224),\n",
    "            verbose=0,\n",
    "            col_names=['input_size', 'output_size','num_params', 'trainable'],\n",
    "            row_settings=['var_names']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 06:21:55.435605: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-15 06:21:55.472740: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-15 06:21:55.472771: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-15 06:21:55.473558: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-15 06:21:55.479115: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-15 06:21:56.242987: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# 1-4 : torch.utils.tensorboard.SummaryWriter를 사용해 Track하도록 train loop 변형\n",
    "\n",
    "# Define train function that can use SummaryWriter\n",
    "import torch\n",
    "import torch.utils.tensorboard\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def train_v1 (\n",
    "        model: torch.nn.Module, \n",
    "        train_dataloader: torch.utils.data.DataLoader, \n",
    "        test_dataloader: torch.utils.data.DataLoader, \n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        loss_fn: torch.nn.Module,\n",
    "        epochs: int,\n",
    "        device: torch.device,\n",
    "        writer: torch.utils.tensorboard.SummaryWriter\n",
    "    ) -> Dict[str, List]:\n",
    "    \n",
    "    \"\"\"Trains and tests a PyTorch model.\n",
    "\n",
    "    Passes a target PyTorch models through train_step() and test_step()\n",
    "    functions for a number of epochs, training and testing the model\n",
    "    in the same epoch loop.\n",
    "\n",
    "    Calculates, prints and stores evaluation metrics throughout.\n",
    "\n",
    "    Args:\n",
    "      model: A PyTorch model to be trained and tested.\n",
    "      train_dataloader: A DataLoader instance for the model to be trained on.\n",
    "      test_dataloader: A DataLoader instance for the model to be tested on.\n",
    "      optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "      loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
    "      epochs: An integer indicating how many epochs to train for.\n",
    "      device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "      \n",
    "    Returns:\n",
    "      A dictionary of training and testing loss as well as training and\n",
    "      testing accuracy metrics. Each metric has a value in a list for \n",
    "      each epoch.\n",
    "      In the form: {train_loss: [...],\n",
    "                train_acc: [...],\n",
    "                test_loss: [...],\n",
    "                test_acc: [...]} \n",
    "      For example if training for epochs=2: \n",
    "              {train_loss: [2.0616, 1.0537],\n",
    "                train_acc: [0.3945, 0.3945],\n",
    "                test_loss: [1.2641, 1.5706],\n",
    "                test_acc: [0.3400, 0.2973]} \n",
    "    \"\"\"\n",
    "    # Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "               \"train_acc\": [],\n",
    "               \"test_loss\": [],\n",
    "               \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    # Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer,\n",
    "                                           device=device)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "                                        dataloader=test_dataloader,\n",
    "                                        loss_fn=loss_fn,\n",
    "                                        device=device)\n",
    "\n",
    "        # Print out what's happening\n",
    "        print(\n",
    "          f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_acc: {train_acc:.4f} | \"\n",
    "          f\"test_loss: {test_loss:.4f} | \"\n",
    "          f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "        ### New: Experiment tracking ###\n",
    "        # Add loss results to SummaryWriter\n",
    "        writer.add_scalars(main_tag=\"Loss\", \n",
    "                           tag_scalar_dict={\"train_loss\": train_loss,\n",
    "                                            \"test_loss\": test_loss},\n",
    "                           global_step=epoch)\n",
    "\n",
    "        # Add accuracy results to SummaryWriter\n",
    "        writer.add_scalars(main_tag=\"Accuracy\", \n",
    "                           tag_scalar_dict={\"train_acc\": train_acc,\n",
    "                                            \"test_acc\": test_acc}, \n",
    "                           global_step=epoch)\n",
    "        \n",
    "        # Track the PyTorch model architecture\n",
    "        writer.add_graph(model=model, \n",
    "                         # Pass in an example input\n",
    "                         input_to_model=torch.randn(32, 3, 224, 224).to(device))\n",
    "    \n",
    "    # Close the writer\n",
    "    writer.close()\n",
    "    \n",
    "    ### End new ###\n",
    "\n",
    "    # Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f087334e5d4d458ca0f881143f5d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0735 | train_acc: 0.4297 | test_loss: 0.7943 | test_acc: 0.8456\n",
      "Epoch: 2 | train_loss: 0.8468 | train_acc: 0.7148 | test_loss: 0.7604 | test_acc: 0.7841\n",
      "Epoch: 3 | train_loss: 0.7873 | train_acc: 0.7266 | test_loss: 0.6967 | test_acc: 0.8551\n",
      "Epoch: 4 | train_loss: 0.6480 | train_acc: 0.7539 | test_loss: 0.5912 | test_acc: 0.8759\n",
      "Epoch: 5 | train_loss: 0.5841 | train_acc: 0.8867 | test_loss: 0.5857 | test_acc: 0.8968\n"
     ]
    }
   ],
   "source": [
    "# Define writer, loss and optimizer and then train\n",
    "from going_modular import *\n",
    "\n",
    "writer = torch.utils.tensorboard.SummaryWriter()\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "results = train_v1(\n",
    "            model=model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            optimizer=optimizer,\n",
    "            loss_fn=loss_func,\n",
    "            epochs=5,\n",
    "            device=device,\n",
    "            writer=writer\n",
    "        )\n",
    "\n",
    "# Ctrl + Shift + P -> Python: Launch TensorBoard 해서 TensorBoard로 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - 2-1 : SummaryWriter 생성 도구 (대략 어느 폴더에 어떻게 잘 눈에 띄게 저장할까)\n",
    "\n",
    "# Create a helper function to build SummaryWriter instances\n",
    "import datetime\n",
    "import os\n",
    "import torch\n",
    "import torch.utils.tensorboard\n",
    "\n",
    "def create_writer(experiment_name: str, \n",
    "                  model_name: str, \n",
    "                  extra: str=None) -> torch.utils.tensorboard.writer.SummaryWriter():\n",
    "    \"\"\"Creates a torch.utils.tensorboard.writer.SummaryWriter() instance saving to a specific log_dir.\n",
    "\n",
    "    log_dir is a combination of runs/timestamp/experiment_name/model_name/extra.\n",
    "\n",
    "    Where timestamp is the current date in YYYY-MM-DD format.\n",
    "\n",
    "    Args:\n",
    "        experiment_name (str): Name of experiment.\n",
    "        model_name (str): Name of model.\n",
    "        extra (str, optional): Anything extra to add to the directory. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        torch.utils.tensorboard.writer.SummaryWriter(): Instance of a writer saving to log_dir.\n",
    "\n",
    "    Example usage:\n",
    "        # Create a writer saving to \"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\"\n",
    "        writer = create_writer(experiment_name=\"data_10_percent\",\n",
    "                               model_name=\"effnetb2\",\n",
    "                               extra=\"5_epochs\")\n",
    "        # The above is the same as:\n",
    "        writer = SummaryWriter(log_dir=\"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\")\n",
    "    \"\"\"\n",
    "    # Get timestamp of current date (all experiments on certain day live in same folder)\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d\") # returns current date in YYYY-MM-DD format\n",
    "\n",
    "    if extra:\n",
    "        # Create log directory path\n",
    "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)\n",
    "    else:\n",
    "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n",
    "        \n",
    "    print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")\n",
    "    return torch.utils.tensorboard.writer.SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결론적으로 봤을때 v1이랑 같다. 뭔가 더 들어갔나 싶었는데 결국 바뀐게 없네.\n",
    "\n",
    "# Define train function that can use SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.utils.tensorboard\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def train_v2 (\n",
    "        model: torch.nn.Module, \n",
    "        train_dataloader: torch.utils.data.DataLoader, \n",
    "        test_dataloader: torch.utils.data.DataLoader, \n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        loss_fn: torch.nn.Module,\n",
    "        epochs: int,\n",
    "        device: torch.device,\n",
    "        writer: torch.utils.tensorboard.SummaryWriter\n",
    "    ) -> Dict[str, List]:\n",
    "    \"\"\"Trains and tests a PyTorch model.\n",
    "\n",
    "    Passes a target PyTorch models through train_step() and test_step()\n",
    "    functions for a number of epochs, training and testing the model\n",
    "    in the same epoch loop.\n",
    "\n",
    "    Calculates, prints and stores evaluation metrics throughout.\n",
    "\n",
    "    Stores metrics to specified writer log_dir if present.\n",
    "\n",
    "    Args:\n",
    "      model: A PyTorch model to be trained and tested.\n",
    "      train_dataloader: A DataLoader instance for the model to be trained on.\n",
    "      test_dataloader: A DataLoader instance for the model to be tested on.\n",
    "      optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "      loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
    "      epochs: An integer indicating how many epochs to train for.\n",
    "      device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "      writer: A SummaryWriter() instance to log model results to.\n",
    "\n",
    "    Returns:\n",
    "      A dictionary of training and testing loss as well as training and\n",
    "      testing accuracy metrics. Each metric has a value in a list for \n",
    "      each epoch.\n",
    "      In the form: {train_loss: [...],\n",
    "                train_acc: [...],\n",
    "                test_loss: [...],\n",
    "                test_acc: [...]} \n",
    "      For example if training for epochs=2: \n",
    "              {train_loss: [2.0616, 1.0537],\n",
    "                train_acc: [0.3945, 0.3945],\n",
    "                test_loss: [1.2641, 1.5706],\n",
    "                test_acc: [0.3400, 0.2973]} \n",
    "    \"\"\"\n",
    "    # Create empty results dictionary\n",
    "    results = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "        }\n",
    "\n",
    "    # Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                          dataloader=train_dataloader,\n",
    "                                          loss_fn=loss_fn,\n",
    "                                          optimizer=optimizer,\n",
    "                                          device=device)\n",
    "        test_loss, test_acc = test_step(\n",
    "            model=model,\n",
    "            dataloader=test_dataloader,\n",
    "            loss_fn=loss_fn,\n",
    "            device=device)\n",
    "\n",
    "        # Print out what's happening\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "\n",
    "        ### New: Use the writer parameter to track experiments ###\n",
    "        # See if there's a writer, if so, log to it\n",
    "        if writer:\n",
    "            # Add results to SummaryWriter\n",
    "            writer.add_scalars(main_tag=\"Loss\", \n",
    "                               tag_scalar_dict={\"train_loss\": train_loss,\n",
    "                                                \"test_loss\": test_loss},\n",
    "                               global_step=epoch)\n",
    "            writer.add_scalars(main_tag=\"Accuracy\", \n",
    "                               tag_scalar_dict={\"train_acc\": train_acc,\n",
    "                                                \"test_acc\": test_acc}, \n",
    "                               global_step=epoch)\n",
    "\n",
    "            # Close the writer\n",
    "            writer.close()\n",
    "        else:\n",
    "            pass\n",
    "    ### End new ###\n",
    "\n",
    "    # Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data/pizza_steak_sushi directory exists, skipping download.\n",
      "train_dir_10: data/pizza_steak_sushi/train\n",
      "[INFO] data/pizza_steak_sushi_20_percent directory exists, skipping download.\n",
      "train_dir_20: data/pizza_steak_sushi_20_percent/train\n",
      "test_dir: data/pizza_steak_sushi/test\n",
      "len(train_dataloader_10.dataset): 225\n",
      "len(train_dataloader_20.dataset): 450\n",
      "len(test_dataloader.dataset): 75\n"
     ]
    }
   ],
   "source": [
    "# - 2-2 : Data download & Transforms 준비 & DataLoader 준비\n",
    "\n",
    "#\n",
    "# Multiple experiments\n",
    "# - 10% data vs 20% data\n",
    "# - efficient_b0 vs efficient_b2\n",
    "# - 5 epochs vs 10 epochs\n",
    "#\n",
    "\n",
    "data_10_path = download_data(\n",
    "                source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
    "                destination=\"pizza_steak_sushi\")\n",
    "\n",
    "train_dir_10 = data_10_path / 'train'\n",
    "print(f'train_dir_10: {train_dir_10}')\n",
    "\n",
    "data_20_path = download_data(\n",
    "                source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
    "                destination=\"pizza_steak_sushi_20_percent\")\n",
    "\n",
    "train_dir_20 = data_20_path / 'train'\n",
    "print(f'train_dir_20: {train_dir_20}')\n",
    "\n",
    "# We use same data for test\n",
    "test_dir = data_10_path / 'test'\n",
    "print(f'test_dir: {test_dir}')\n",
    "\n",
    "# Create dataloaders\n",
    "# Create common transforms\n",
    "#\n",
    "# Note: EfficientB0와 EfficientB2의 DEFAULT transform은 사실 다르다\n",
    "#       근데 메모리가 많이 들기도 하고 책에서도 그냥 똑같은 크기로 바꿔 버리니까 여기선\n",
    "#       일단 그렇게 가기로 한다\n",
    "#\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "simple_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize( mean = [0.485, 0.456, 0.406],  \n",
    "                                       std = [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataloader_10, test_dataloader, class_names = create_dataloaders(\n",
    "                                                        train_dir=train_dir_10,\n",
    "                                                        test_dir=test_dir,\n",
    "                                                        transform=simple_transform,\n",
    "                                                        batch_size=BATCH_SIZE\n",
    "                                                    )\n",
    "\n",
    "train_dataloader_20, _, _ = create_dataloaders(\n",
    "                                train_dir=train_dir_20,\n",
    "                                test_dir=test_dir,\n",
    "                                transform=simple_transform,\n",
    "                                batch_size=BATCH_SIZE\n",
    "                            )\n",
    "\n",
    "print(f'len(train_dataloader_10.dataset): {len(train_dataloader_10.dataset)}')\n",
    "print(f'len(train_dataloader_20.dataset): {len(train_dataloader_20.dataset)}')\n",
    "print(f'len(test_dataloader.dataset): {len(test_dataloader.dataset)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - 2-3 : Model 생성 도구\n",
    "\n",
    "# Create model creation function\n",
    "import torchinfo\n",
    "\n",
    "def create_effnetb0(out_features:int):\n",
    "    weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "    model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n",
    "\n",
    "    # Freeze base layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Change the classifier\n",
    "    model.classifier = torch.nn.Sequential(\n",
    "        torch.nn.Dropout(0.2, inplace=True),\n",
    "        torch.nn.Linear(in_features=1280,\n",
    "                        out_features=out_features,\n",
    "                        bias=True)\n",
    "    ).to(device)\n",
    "\n",
    "    model.name = 'effnetb0'\n",
    "    return model\n",
    "\n",
    "def create_effnetb2(out_features:int):\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    model = torchvision.models.efficientnet_b2(weights=weights).to(device)\n",
    "\n",
    "    # Freeze base layers\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Change the classifier\n",
    "    model.classifier = torch.nn.Sequential(\n",
    "        torch.nn.Dropout(0.2, inplace=True),\n",
    "        torch.nn.Linear(in_features=1408, # torchinfo.summary로 확인\n",
    "                        out_features=out_features,\n",
    "                        bias=True)\n",
    "    ).to(device)\n",
    "\n",
    "    model.name = 'effnetb2'\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "EfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 3]              --                   Partial\n",
       "├─Sequential (features)                                      [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   False\n",
       "│    └─Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   False\n",
       "│    │    └─Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   (864)                False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   (64)                 False\n",
       "│    │    └─SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\n",
       "│    └─Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   (1,448)              False\n",
       "│    └─Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     (6,004)              False\n",
       "│    │    └─MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     (10,710)             False\n",
       "│    └─Sequential (3)                                        [32, 24, 56, 56]     [32, 40, 28, 28]     --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 24, 56, 56]     [32, 40, 28, 28]     (15,350)             False\n",
       "│    │    └─MBConv (1)                                       [32, 40, 28, 28]     [32, 40, 28, 28]     (31,290)             False\n",
       "│    └─Sequential (4)                                        [32, 40, 28, 28]     [32, 80, 14, 14]     --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 40, 28, 28]     [32, 80, 14, 14]     (37,130)             False\n",
       "│    │    └─MBConv (1)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n",
       "│    │    └─MBConv (2)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n",
       "│    └─Sequential (5)                                        [32, 80, 14, 14]     [32, 112, 14, 14]    --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 80, 14, 14]     [32, 112, 14, 14]    (126,004)            False\n",
       "│    │    └─MBConv (1)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n",
       "│    │    └─MBConv (2)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n",
       "│    └─Sequential (6)                                        [32, 112, 14, 14]    [32, 192, 7, 7]      --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 112, 14, 14]    [32, 192, 7, 7]      (262,492)            False\n",
       "│    │    └─MBConv (1)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
       "│    │    └─MBConv (2)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
       "│    │    └─MBConv (3)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
       "│    └─Sequential (7)                                        [32, 192, 7, 7]      [32, 320, 7, 7]      --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 192, 7, 7]      [32, 320, 7, 7]      (717,232)            False\n",
       "│    └─Conv2dNormActivation (8)                              [32, 320, 7, 7]      [32, 1280, 7, 7]     --                   False\n",
       "│    │    └─Conv2d (0)                                       [32, 320, 7, 7]      [32, 1280, 7, 7]     (409,600)            False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 1280, 7, 7]     [32, 1280, 7, 7]     (2,560)              False\n",
       "│    │    └─SiLU (2)                                         [32, 1280, 7, 7]     [32, 1280, 7, 7]     --                   --\n",
       "├─AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]     [32, 1280, 1, 1]     --                   --\n",
       "├─Sequential (classifier)                                    [32, 1280]           [32, 3]              --                   True\n",
       "│    └─Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\n",
       "│    └─Linear (1)                                            [32, 1280]           [32, 3]              3,843                True\n",
       "============================================================================================================================================\n",
       "Total params: 4,011,391\n",
       "Trainable params: 3,843\n",
       "Non-trainable params: 4,007,548\n",
       "Total mult-adds (Units.GIGABYTES): 12.31\n",
       "============================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 3452.09\n",
       "Params size (MB): 16.05\n",
       "Estimated Total Size (MB): 3487.41\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effnetb0 = create_effnetb0(len(class_names))\n",
    "torchinfo.summary(\n",
    "            model = effnetb0, \n",
    "            input_size=(32, 3, 224, 224), \n",
    "            col_names = [\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "            col_width = 20,\n",
    "            row_settings = ['var_names']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "EfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 3]              --                   Partial\n",
       "├─Sequential (features)                                      [32, 3, 224, 224]    [32, 1408, 7, 7]     --                   False\n",
       "│    └─Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   False\n",
       "│    │    └─Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   (864)                False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   (64)                 False\n",
       "│    │    └─SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\n",
       "│    └─Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   (1,448)              False\n",
       "│    │    └─MBConv (1)                                       [32, 16, 112, 112]   [32, 16, 112, 112]   (612)                False\n",
       "│    └─Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     (6,004)              False\n",
       "│    │    └─MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     (10,710)             False\n",
       "│    │    └─MBConv (2)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     (10,710)             False\n",
       "│    └─Sequential (3)                                        [32, 24, 56, 56]     [32, 48, 28, 28]     --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 24, 56, 56]     [32, 48, 28, 28]     (16,518)             False\n",
       "│    │    └─MBConv (1)                                       [32, 48, 28, 28]     [32, 48, 28, 28]     (43,308)             False\n",
       "│    │    └─MBConv (2)                                       [32, 48, 28, 28]     [32, 48, 28, 28]     (43,308)             False\n",
       "│    └─Sequential (4)                                        [32, 48, 28, 28]     [32, 88, 14, 14]     --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 48, 28, 28]     [32, 88, 14, 14]     (50,300)             False\n",
       "│    │    └─MBConv (1)                                       [32, 88, 14, 14]     [32, 88, 14, 14]     (123,750)            False\n",
       "│    │    └─MBConv (2)                                       [32, 88, 14, 14]     [32, 88, 14, 14]     (123,750)            False\n",
       "│    │    └─MBConv (3)                                       [32, 88, 14, 14]     [32, 88, 14, 14]     (123,750)            False\n",
       "│    └─Sequential (5)                                        [32, 88, 14, 14]     [32, 120, 14, 14]    --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 88, 14, 14]     [32, 120, 14, 14]    (149,158)            False\n",
       "│    │    └─MBConv (1)                                       [32, 120, 14, 14]    [32, 120, 14, 14]    (237,870)            False\n",
       "│    │    └─MBConv (2)                                       [32, 120, 14, 14]    [32, 120, 14, 14]    (237,870)            False\n",
       "│    │    └─MBConv (3)                                       [32, 120, 14, 14]    [32, 120, 14, 14]    (237,870)            False\n",
       "│    └─Sequential (6)                                        [32, 120, 14, 14]    [32, 208, 7, 7]      --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 120, 14, 14]    [32, 208, 7, 7]      (301,406)            False\n",
       "│    │    └─MBConv (1)                                       [32, 208, 7, 7]      [32, 208, 7, 7]      (686,868)            False\n",
       "│    │    └─MBConv (2)                                       [32, 208, 7, 7]      [32, 208, 7, 7]      (686,868)            False\n",
       "│    │    └─MBConv (3)                                       [32, 208, 7, 7]      [32, 208, 7, 7]      (686,868)            False\n",
       "│    │    └─MBConv (4)                                       [32, 208, 7, 7]      [32, 208, 7, 7]      (686,868)            False\n",
       "│    └─Sequential (7)                                        [32, 208, 7, 7]      [32, 352, 7, 7]      --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 208, 7, 7]      [32, 352, 7, 7]      (846,900)            False\n",
       "│    │    └─MBConv (1)                                       [32, 352, 7, 7]      [32, 352, 7, 7]      (1,888,920)          False\n",
       "│    └─Conv2dNormActivation (8)                              [32, 352, 7, 7]      [32, 1408, 7, 7]     --                   False\n",
       "│    │    └─Conv2d (0)                                       [32, 352, 7, 7]      [32, 1408, 7, 7]     (495,616)            False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 1408, 7, 7]     [32, 1408, 7, 7]     (2,816)              False\n",
       "│    │    └─SiLU (2)                                         [32, 1408, 7, 7]     [32, 1408, 7, 7]     --                   --\n",
       "├─AdaptiveAvgPool2d (avgpool)                                [32, 1408, 7, 7]     [32, 1408, 1, 1]     --                   --\n",
       "├─Sequential (classifier)                                    [32, 1408]           [32, 3]              --                   True\n",
       "│    └─Dropout (0)                                           [32, 1408]           [32, 1408]           --                   --\n",
       "│    └─Linear (1)                                            [32, 1408]           [32, 3]              4,227                True\n",
       "============================================================================================================================================\n",
       "Total params: 7,705,221\n",
       "Trainable params: 4,227\n",
       "Non-trainable params: 7,700,994\n",
       "Total mult-adds (Units.GIGABYTES): 21.04\n",
       "============================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 5017.53\n",
       "Params size (MB): 30.82\n",
       "Estimated Total Size (MB): 5067.62\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effnetb2 = create_effnetb2(len(class_names))\n",
    "torchinfo.summary(\n",
    "            model = effnetb2, \n",
    "            input_size=(32, 3, 224, 224), \n",
    "            col_names = [\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "            col_width = 20,\n",
    "            row_settings = ['var_names']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Experiment number: 1\n",
      "[INFO] Model: effnetb0\n",
      "[INFO] DataLoader: data_10_percent\n",
      "[INFO] Number of epochs: 5\n",
      "[INFO] Created SummaryWriter, saving to: runs/2024-01-15/data_10_percent/effnetb0/5_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ee0df9bca34858b0f87da3ef2a907d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0508 | train_acc: 0.3945 | test_loss: 0.8675 | test_acc: 0.6705\n",
      "Epoch: 2 | train_loss: 0.9289 | train_acc: 0.6133 | test_loss: 0.6877 | test_acc: 0.8854\n",
      "Epoch: 3 | train_loss: 0.8018 | train_acc: 0.7148 | test_loss: 0.6782 | test_acc: 0.8561\n",
      "Epoch: 4 | train_loss: 0.6824 | train_acc: 0.7812 | test_loss: 0.6827 | test_acc: 0.8258\n",
      "Epoch: 5 | train_loss: 0.6040 | train_acc: 0.9023 | test_loss: 0.6434 | test_acc: 0.8665\n",
      "[INFO] Saving model to: models/07_effnetb0_data_10_percent_5_epochs.pth\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment number: 2\n",
      "[INFO] Model: effnetb2\n",
      "[INFO] DataLoader: data_10_percent\n",
      "[INFO] Number of epochs: 5\n",
      "[INFO] Created SummaryWriter, saving to: runs/2024-01-15/data_10_percent/effnetb2/5_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a295f0134b554b51ab3ed644583b26be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0354 | train_acc: 0.3828 | test_loss: 0.9575 | test_acc: 0.6203\n",
      "Epoch: 2 | train_loss: 0.9243 | train_acc: 0.6094 | test_loss: 0.7848 | test_acc: 0.8864\n",
      "Epoch: 3 | train_loss: 0.7670 | train_acc: 0.7578 | test_loss: 0.7605 | test_acc: 0.7538\n",
      "Epoch: 4 | train_loss: 0.6949 | train_acc: 0.9062 | test_loss: 0.6795 | test_acc: 0.8968\n",
      "Epoch: 5 | train_loss: 0.6208 | train_acc: 0.8789 | test_loss: 0.6831 | test_acc: 0.8665\n",
      "[INFO] Saving model to: models/07_effnetb2_data_10_percent_5_epochs.pth\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment number: 3\n",
      "[INFO] Model: effnetb0\n",
      "[INFO] DataLoader: data_10_percent\n",
      "[INFO] Number of epochs: 10\n",
      "[INFO] Created SummaryWriter, saving to: runs/2024-01-15/data_10_percent/effnetb0/10_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ab7e9419e44abd99435efdc2223dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0879 | train_acc: 0.3867 | test_loss: 0.8710 | test_acc: 0.7348\n",
      "Epoch: 2 | train_loss: 0.8636 | train_acc: 0.6836 | test_loss: 0.8188 | test_acc: 0.7642\n",
      "Epoch: 3 | train_loss: 0.7992 | train_acc: 0.7266 | test_loss: 0.7524 | test_acc: 0.7746\n",
      "Epoch: 4 | train_loss: 0.7497 | train_acc: 0.7578 | test_loss: 0.6796 | test_acc: 0.8362\n",
      "Epoch: 5 | train_loss: 0.6206 | train_acc: 0.7500 | test_loss: 0.5715 | test_acc: 0.9280\n",
      "Epoch: 6 | train_loss: 0.6677 | train_acc: 0.7500 | test_loss: 0.5280 | test_acc: 0.8655\n",
      "Epoch: 7 | train_loss: 0.5803 | train_acc: 0.7500 | test_loss: 0.5141 | test_acc: 0.9176\n",
      "Epoch: 8 | train_loss: 0.4844 | train_acc: 0.9570 | test_loss: 0.5601 | test_acc: 0.8570\n",
      "Epoch: 9 | train_loss: 0.4709 | train_acc: 0.8984 | test_loss: 0.5359 | test_acc: 0.8977\n",
      "Epoch: 10 | train_loss: 0.5876 | train_acc: 0.7891 | test_loss: 0.5115 | test_acc: 0.8977\n",
      "[INFO] Saving model to: models/07_effnetb0_data_10_percent_10_epochs.pth\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment number: 4\n",
      "[INFO] Model: effnetb2\n",
      "[INFO] DataLoader: data_10_percent\n",
      "[INFO] Number of epochs: 10\n",
      "[INFO] Created SummaryWriter, saving to: runs/2024-01-15/data_10_percent/effnetb2/10_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c4e8c4d21c64353a131c62ae05d7b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0220 | train_acc: 0.5469 | test_loss: 0.9599 | test_acc: 0.6411\n",
      "Epoch: 2 | train_loss: 0.9607 | train_acc: 0.5195 | test_loss: 0.9002 | test_acc: 0.6203\n",
      "Epoch: 3 | train_loss: 0.7905 | train_acc: 0.6719 | test_loss: 0.7742 | test_acc: 0.8144\n",
      "Epoch: 4 | train_loss: 0.6913 | train_acc: 0.8828 | test_loss: 0.7265 | test_acc: 0.8153\n",
      "Epoch: 5 | train_loss: 0.5895 | train_acc: 0.8750 | test_loss: 0.6924 | test_acc: 0.8769\n",
      "Epoch: 6 | train_loss: 0.6756 | train_acc: 0.7734 | test_loss: 0.6588 | test_acc: 0.8665\n",
      "Epoch: 7 | train_loss: 0.5569 | train_acc: 0.9258 | test_loss: 0.6020 | test_acc: 0.8864\n",
      "Epoch: 8 | train_loss: 0.5455 | train_acc: 0.8086 | test_loss: 0.6010 | test_acc: 0.8665\n",
      "Epoch: 9 | train_loss: 0.4568 | train_acc: 0.9531 | test_loss: 0.5309 | test_acc: 0.8968\n",
      "Epoch: 10 | train_loss: 0.5536 | train_acc: 0.7930 | test_loss: 0.5039 | test_acc: 0.8864\n",
      "[INFO] Saving model to: models/07_effnetb2_data_10_percent_10_epochs.pth\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment number: 5\n",
      "[INFO] Model: effnetb0\n",
      "[INFO] DataLoader: data_20_percent\n",
      "[INFO] Number of epochs: 5\n",
      "[INFO] Created SummaryWriter, saving to: runs/2024-01-15/data_20_percent/effnetb0/5_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8e6974ff074d29ac17cd57aa6a8bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9367 | train_acc: 0.5896 | test_loss: 0.7189 | test_acc: 0.8258\n",
      "Epoch: 2 | train_loss: 0.6751 | train_acc: 0.8333 | test_loss: 0.5597 | test_acc: 0.9176\n",
      "Epoch: 3 | train_loss: 0.5540 | train_acc: 0.8354 | test_loss: 0.4823 | test_acc: 0.9176\n",
      "Epoch: 4 | train_loss: 0.4760 | train_acc: 0.8958 | test_loss: 0.4509 | test_acc: 0.8968\n",
      "Epoch: 5 | train_loss: 0.4791 | train_acc: 0.8729 | test_loss: 0.3763 | test_acc: 0.9384\n",
      "[INFO] Saving model to: models/07_effnetb0_data_20_percent_5_epochs.pth\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment number: 6\n",
      "[INFO] Model: effnetb2\n",
      "[INFO] DataLoader: data_20_percent\n",
      "[INFO] Number of epochs: 5\n",
      "[INFO] Created SummaryWriter, saving to: runs/2024-01-15/data_20_percent/effnetb2/5_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c329d706a094921a36916eaec9b2233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9404 | train_acc: 0.5938 | test_loss: 0.7761 | test_acc: 0.8561\n",
      "Epoch: 2 | train_loss: 0.6945 | train_acc: 0.8271 | test_loss: 0.6328 | test_acc: 0.8968\n",
      "Epoch: 3 | train_loss: 0.5672 | train_acc: 0.8479 | test_loss: 0.5496 | test_acc: 0.9176\n",
      "Epoch: 4 | train_loss: 0.5010 | train_acc: 0.8688 | test_loss: 0.5615 | test_acc: 0.8873\n",
      "Epoch: 5 | train_loss: 0.4411 | train_acc: 0.8646 | test_loss: 0.4853 | test_acc: 0.8873\n",
      "[INFO] Saving model to: models/07_effnetb2_data_20_percent_5_epochs.pth\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment number: 7\n",
      "[INFO] Model: effnetb0\n",
      "[INFO] DataLoader: data_20_percent\n",
      "[INFO] Number of epochs: 10\n",
      "[INFO] Created SummaryWriter, saving to: runs/2024-01-15/data_20_percent/effnetb0/10_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c492ad95e09461988a47a27a50ae53b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.9932 | train_acc: 0.4917 | test_loss: 0.7131 | test_acc: 0.8665\n",
      "Epoch: 2 | train_loss: 0.7070 | train_acc: 0.8313 | test_loss: 0.5754 | test_acc: 0.8968\n",
      "Epoch: 3 | train_loss: 0.5647 | train_acc: 0.8771 | test_loss: 0.4591 | test_acc: 0.8968\n",
      "Epoch: 4 | train_loss: 0.5577 | train_acc: 0.8479 | test_loss: 0.4023 | test_acc: 0.9072\n",
      "Epoch: 5 | train_loss: 0.4622 | train_acc: 0.8625 | test_loss: 0.3850 | test_acc: 0.9176\n",
      "Epoch: 6 | train_loss: 0.4233 | train_acc: 0.8875 | test_loss: 0.3563 | test_acc: 0.9072\n",
      "Epoch: 7 | train_loss: 0.4194 | train_acc: 0.8688 | test_loss: 0.3299 | test_acc: 0.9489\n",
      "Epoch: 8 | train_loss: 0.4367 | train_acc: 0.8646 | test_loss: 0.2972 | test_acc: 0.8968\n",
      "Epoch: 9 | train_loss: 0.3188 | train_acc: 0.9271 | test_loss: 0.3288 | test_acc: 0.9280\n",
      "Epoch: 10 | train_loss: 0.3019 | train_acc: 0.9271 | test_loss: 0.2712 | test_acc: 0.9280\n",
      "[INFO] Saving model to: models/07_effnetb0_data_20_percent_10_epochs.pth\n",
      "--------------------------------------------------\n",
      "\n",
      "[INFO] Experiment number: 8\n",
      "[INFO] Model: effnetb2\n",
      "[INFO] DataLoader: data_20_percent\n",
      "[INFO] Number of epochs: 10\n",
      "[INFO] Created SummaryWriter, saving to: runs/2024-01-15/data_20_percent/effnetb2/10_epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "528654f5a53e4f308b0ace03165a27c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0049 | train_acc: 0.5062 | test_loss: 0.7858 | test_acc: 0.8968\n",
      "Epoch: 2 | train_loss: 0.7413 | train_acc: 0.8021 | test_loss: 0.6790 | test_acc: 0.8769\n",
      "Epoch: 3 | train_loss: 0.6294 | train_acc: 0.8396 | test_loss: 0.5692 | test_acc: 0.9176\n",
      "Epoch: 4 | train_loss: 0.4857 | train_acc: 0.8854 | test_loss: 0.5377 | test_acc: 0.9176\n",
      "Epoch: 5 | train_loss: 0.4874 | train_acc: 0.8688 | test_loss: 0.4784 | test_acc: 0.9280\n",
      "Epoch: 6 | train_loss: 0.4321 | train_acc: 0.8938 | test_loss: 0.4305 | test_acc: 0.9176\n",
      "Epoch: 7 | train_loss: 0.3883 | train_acc: 0.8896 | test_loss: 0.4567 | test_acc: 0.9176\n",
      "Epoch: 8 | train_loss: 0.3447 | train_acc: 0.9229 | test_loss: 0.4000 | test_acc: 0.9176\n",
      "Epoch: 9 | train_loss: 0.3064 | train_acc: 0.9313 | test_loss: 0.4175 | test_acc: 0.8977\n",
      "Epoch: 10 | train_loss: 0.3440 | train_acc: 0.9250 | test_loss: 0.4031 | test_acc: 0.9176\n",
      "[INFO] Saving model to: models/07_effnetb2_data_20_percent_10_epochs.pth\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "- 2-4 : 여러 다른 조건에서 테스트 후 TensorBoard로 보기\n",
    "\n",
    "# Create experiments and set up training code\n",
    "# 1. Create epochs list\n",
    "num_epochs = [5, 10]\n",
    "\n",
    "# 2. Create models list (need to create a new model for each experiment)\n",
    "models = [\"effnetb0\", \"effnetb2\"]\n",
    "\n",
    "# 3. Create dataloaders dictionary for various dataloaders\n",
    "train_dataloaders = {\"data_10_percent\": train_dataloader_10,\n",
    "                     \"data_20_percent\": train_dataloader_20}\n",
    "\n",
    "# Keep track of experiment numbers\n",
    "experiment_number = 0\n",
    "\n",
    "# 3. Loop through each DataLoader\n",
    "for dataloader_name, train_dataloader in train_dataloaders.items():\n",
    "\n",
    "    # 4. Loop through each number of epochs\n",
    "    for epochs in num_epochs: \n",
    "\n",
    "        # 5. Loop through each model name and create a new model based on the name\n",
    "        for model_name in models:\n",
    "\n",
    "            # 6. Create information print outs\n",
    "            experiment_number += 1\n",
    "            print(f\"[INFO] Experiment number: {experiment_number}\")\n",
    "            print(f\"[INFO] Model: {model_name}\")\n",
    "            print(f\"[INFO] DataLoader: {dataloader_name}\")\n",
    "            print(f\"[INFO] Number of epochs: {epochs}\")  \n",
    "\n",
    "            # 7. Select the model\n",
    "            if model_name == \"effnetb0\":\n",
    "                model = create_effnetb0(len(class_names)) # creates a new model each time (important because we want each experiment to start from scratch)\n",
    "            else:\n",
    "                model = create_effnetb2(len(class_names)) # creates a new model each time (important because we want each experiment to start from scratch)\n",
    "            \n",
    "            # 8. Create a new loss and optimizer for every model\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "\n",
    "            # 9. Train target model with target dataloaders and track experiments\n",
    "            train_v2(\n",
    "                model=model,\n",
    "                train_dataloader=train_dataloader,\n",
    "                test_dataloader=test_dataloader, \n",
    "                optimizer=optimizer,\n",
    "                loss_fn=loss_fn,\n",
    "                epochs=epochs,\n",
    "                device=device,\n",
    "                writer=create_writer(experiment_name=dataloader_name,\n",
    "                                     model_name=model_name,\n",
    "                                     extra=f\"{epochs}_epochs\"))\n",
    "            \n",
    "            # 10. Save the model to file so we can get back the best model\n",
    "            save_filepath = f\"07_{model_name}_{dataloader_name}_{epochs}_epochs.pth\"\n",
    "            save_model(model=model,\n",
    "                       target_dir=\"models\",\n",
    "                       model_name=save_filepath)\n",
    "            print(\"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
