{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier Performance Measures\n",
    "\n",
    "- Regressor야 prediction과 true사이에 값이 범위로 나올테니 evaluate하는게 쉬운 편이지만 (그냥 cost minimization) classifier의 경우 true/false를 갖고 판단하는 거니까 좀 더 생각해 볼만한 여지가 있는 것들이 있다\n",
    "\n",
    "## 너무 Skew 되어 있으면 어떻게 해야 하나?\n",
    "- 좀 극단적으로 말해서 True로 분류될 수 있는 케이스가 90%이고 False로 분류될 수 있는 케이스는 10% 뿐인 데이터가 있다고 해보자\n",
    "- 여기에서 좀 멍청하게 만들면 모든 것을 True라고 말하는 Classifier를 만들수도 있는데, 이 경우에도 적중률은 90%나 되게 된다\n",
    "- 이러한 classifier를 좋은 classifier라고 할 수 있는 것일까?\n",
    "- 혹은 90%가 False이고 10%만 True인 case에서 모든 것을 False라고 하는 classifier도 생각해 볼 수 있다. 이 경우에도 classifier는 90% 확률로 진실(=False)을 말하게 된다\n",
    "\n",
    "## Precision과 recall\n",
    "- Precision = TruePositive/(TruePositive + FalsePositive)\n",
    "- Recall = TruePositive/(TruePositive + FalseNegative)\n",
    "- Precision은 '내가 True라고 한 것 중에 진짜 True인 비율'을 의미한다\n",
    "- Recall은 '진짜 True인 case중에 내가 True를 몇개나 했나'를 의미한다\n",
    "- Precision-Recall Trade-off란 이런거다\n",
    "   - Precision을 높이고 싶다면 threshold를 높게 잡아 좀더 자신있는 것들만 True라고 하고 아닌건 False라고 하면 된다. 이 경우 사실 True인데 True가 아니라고 말할 확률이 낮아지니 Recall이 낮아진다\n",
    "   - Recall을 높이고 싶다면 threshold를 낮게 잡아 좀더 많은 것을 True라고 하게 하면 된다. 이렇게 하면 진짜 True이든 아니든 True라고 하게 될 확률이 높아지니 Recall이 높아지지만 그렇게 True라고 한 것 중에 False positive도 높아질테니 Precision은 낮아진다\n",
    "\n",
    "## 흠...\n",
    "- 근데 위의 case를 놓고 다시 생각해 보면 좀 의문이 든다\n",
    "- 90% 확률로 True인 곳에서 100%확률로 True라고 말하는 classifier를 사용하면 precision도 recall도 준수하게 나온다\n",
    "   - 여기에서 TP = 9, FP = 1, TN = 0, FN = 0 이 되므로 Precision = 9/(9+1) = 0.9, Recall = 9/(9+0) = 1 이 된다\n",
    "- 근데 생각해 보면 이러면 classifier로써 별로 의미가 없다...\n",
    "- False와 True를 뒤집어서 생각해보면 이렇다\n",
    "    - TP = 0, FP = 0, TN = 9, FN = 1\n",
    "    - Precision = 0/(0+0) = undefined, Recall = (0/(0+1)) = 0\n",
    "- 즉 뭔가 이상하긴 한데 그냥 사전적 의미의 precision, recall만 보면 괜찮아 보이는 효과가 난다. 으음...\n",
    "\n",
    "# ROC curve\n",
    "- 책에서 위의 문제를 직접적으로 다루지는 않는데 관련된게 하나 나온다. Receiver-Operating Characteristic이 그것이다\n",
    "- 이건 False Positive Rate와 True Positive Rate(=recall)의 상관관계를 그래프로 나타내는 것이다\n",
    "- False Positive Rate는 FP/(FP+TN)로 표현 되는 것 같다. 참고로 True Negative Rate(specificity)는 TN/(TN+FP) 이고\n",
    "- 뭐 저 그래프는 대략 False Positive Rate가 올라갈수록 Recall(=True Positive Rate)도 올라가는데 (왜냐하면 개나소나 다 Positive로 분류되므로) classifier를 똑바로 만들었다면 어느 부분에 변곡점이 생길거고 뭐 그쯤에 끊으면 된다... 같은 얘기인 것 같다\n",
    "- 쬐끔 낫긴 한데 여전히 문제를 제대로 해결하는건 아닌 느낌?\n",
    "\n",
    "# Youden index\n",
    "- Recall을 Sensitivity라고도 부르는 것 같다. 즉 True Positive에 얼마나 Sensitive하냐 라는 뜻인듯\n",
    "- Specificity는 True Negative Rate. 즉 Negative에 대한 Recall로 얼마나 negative를 잘 잡아냈냐\n",
    "- Youden index = Sensitivity + Specificity - 1\n",
    "- 차라리 이게 정확도를 하나로 표현하는데 도움이 되는 것 같은데...\n",
    "\n",
    "# AUC\n",
    "- Area Under Curve\n",
    "- 위에서 말한 ROC Curve의 면적을 의미한다\n",
    "- 이상적인 경우엔 True Positive (Y축) 가 1일때 False Positive (X축)는 0일 것이므로 1이 나온다\n",
    "- 이상적이지 않으면 그 아래로 낮아지는 것이고\n",
    "- AUC에서 이상적인 부분을 찾는데 Youden index가 maximize 되는 곳을 찾는다고 한다\n",
    "- 음.... 근데 그래도 이게 저런 케이스를 찾아내고 잡는데 도움이 되는건지는 잘 몰?루"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
