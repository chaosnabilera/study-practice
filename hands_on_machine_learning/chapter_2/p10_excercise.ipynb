{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "- Try a support vector machine regressor (sklearn.svm.SVR) with various hyperparameters, such as kernel=\"linear\" (with various values for the\n",
    "C hyperparameter) or kernel=\"rbf\" (with various values for the C and gammahyperparameters). \n",
    "- Note that support vector machines don’t scale well to large datasets, so you should probably train your model on just the first 5,000 instances\n",
    "of the training set and use only 3-fold cross-validation, or else it will take hours.\n",
    "- Don’t worry about what the hyperparameters mean for now; we’ll discuss them in Chapter 5. How does the best SVR predictor perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16512, 24)\n",
      "['bedrooms__ratio' 'rooms_per_house__ratio' 'people_per_house__ratio'\n",
      " 'log__total_bedrooms' 'log__total_rooms' 'log__population'\n",
      " 'log__households' 'log__median_income' 'geo__Cluster 0 similarity'\n",
      " 'geo__Cluster 1 similarity' 'geo__Cluster 2 similarity'\n",
      " 'geo__Cluster 3 similarity' 'geo__Cluster 4 similarity'\n",
      " 'geo__Cluster 5 similarity' 'geo__Cluster 6 similarity'\n",
      " 'geo__Cluster 7 similarity' 'geo__Cluster 8 similarity'\n",
      " 'geo__Cluster 9 similarity' 'cat__ocean_proximity_<1H OCEAN'\n",
      " 'cat__ocean_proximity_INLAND' 'cat__ocean_proximity_ISLAND'\n",
      " 'cat__ocean_proximity_NEAR BAY' 'cat__ocean_proximity_NEAR OCEAN'\n",
      " 'remainder__housing_median_age']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ctint\\AppData\\Local\\Temp\\ipykernel_7352\\572255848.py:17: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  idataframe['income_cat'] = pandas.cut(idataframe['median_income'], bins=[0., 1.5, 3.0, 4.5, 6., pandas.np.inf], labels=[1, 2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "# 좀 더 책 스타일로 복습\n",
    "\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler\n",
    "\n",
    "def stratified_sampling_income_category(idataframe):\n",
    "    idataframe = idataframe.copy()\n",
    "    idataframe['income_cat'] = pandas.cut(idataframe['median_income'], bins=[0., 1.5, 3.0, 4.5, 6., pandas.np.inf], labels=[1, 2, 3, 4, 5])\n",
    "    s_train, s_test = train_test_split(idataframe, test_size=0.2, random_state=42, stratify=idataframe['income_cat'])\n",
    "    s_train.drop('income_cat', axis=1, inplace=True)\n",
    "    s_test.drop('income_cat', axis=1, inplace=True)\n",
    "    return s_train, s_test\n",
    "\n",
    "# load data as panda DataFrame\n",
    "idataframe = pandas.read_csv('datasets/housing/housing.csv')\n",
    "\n",
    "# Make stratified sampling of train and test set\n",
    "train_set, test_set = stratified_sampling_income_category(idataframe)\n",
    "\n",
    "# separate label and predictors\n",
    "train_set_predictor = train_set.drop('median_house_value', axis=1)\n",
    "train_set_labels = train_set['median_house_value'].copy()\n",
    "\n",
    "# create a pipeline for preprocessing\n",
    "num_pipeline = make_pipeline(SimpleImputer(strategy='median'), StandardScaler())\n",
    "cat_pipeline = make_pipeline(SimpleImputer(strategy='most_frequent'), OneHotEncoder(handle_unknown='ignore'))\n",
    "\n",
    "def column_ratio(X):\n",
    "    return X[:,[0]] / X[:,[1]]\n",
    "\n",
    "def ratio_name(function_transformer, feature_names_in):\n",
    "    return ['ratio']\n",
    "\n",
    "def ratio_pipeline():\n",
    "    return make_pipeline( SimpleImputer(strategy='median'), FunctionTransformer(column_ratio, feature_names_out=ratio_name), StandardScaler() )\n",
    "\n",
    "class ClusterSimilarity(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.gamma = gamma\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        self.kmeans_ = KMeans(self.n_clusters, n_init='auto', random_state=self.random_state)\n",
    "        self.kmeans_.fit(X, sample_weight=sample_weight)\n",
    "        return self  # always return self!\n",
    "\n",
    "    def transform(self, X):\n",
    "        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n",
    "    \n",
    "    def get_feature_names_out(self, names=None):\n",
    "        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]\n",
    "\n",
    "log_pipeline = make_pipeline( SimpleImputer(strategy='median'), FunctionTransformer(numpy.log, feature_names_out='one-to-one'), StandardScaler() )\n",
    "cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1.0)\n",
    "default_num_pipeline = make_pipeline( SimpleImputer(strategy='median'), StandardScaler() )\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('bedrooms', ratio_pipeline(), ['total_bedrooms', 'total_rooms']),\n",
    "    ('rooms_per_house', ratio_pipeline(), ['total_rooms', 'households']),\n",
    "    ('people_per_house', ratio_pipeline(), ['population', 'households']),\n",
    "    ('log', log_pipeline, ['total_bedrooms', 'total_rooms', 'population','households','median_income']),\n",
    "    ('geo', cluster_simil, ['latitude','longitude']),\n",
    "    ('cat', cat_pipeline, make_column_selector(dtype_include=object)),\n",
    "], remainder=default_num_pipeline) # one column remaining : housing_median_age\n",
    "\n",
    "predictor_prepared = preprocessing.fit_transform(train_set_predictor)\n",
    "\n",
    "print(predictor_prepared.shape)\n",
    "print(preprocessing.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR Linear RMSE : 111428.99585338461\n",
      "SVR RBF RMSE : 118051.51502911921\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine을 써 본다\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "svr_lin = make_pipeline(preprocessing, SVR(kernel='linear'))\n",
    "svr_rbf = make_pipeline(preprocessing, SVR(kernel='rbf'))\n",
    "# svr_reg = make_pipeline(preprocessing, SVR(kernel='rbf', gamma=0.1, C=1.0))\n",
    "svr_lin.fit(train_set_predictor, train_set_labels)\n",
    "svr_rbf.fit(train_set_predictor, train_set_labels)\n",
    "\n",
    "svr_lin_prediction = svr_lin.predict(train_set_predictor)\n",
    "svr_rbf_prediction = svr_rbf.predict(train_set_predictor)\n",
    "\n",
    "svr_lin_rmse = mean_squared_error(train_set_labels, svr_lin_prediction, squared=False)\n",
    "svr_rbf_rmse = mean_squared_error(train_set_labels, svr_rbf_prediction, squared=False)\n",
    "\n",
    "# 둘 다 별 차이 없...\n",
    "print(f\"SVR Linear RMSE : {svr_lin_rmse}\")\n",
    "print(f\"SVR RBF RMSE : {svr_rbf_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'svr__C': 1000.0, 'svr__gamma': 0.1, 'svr__kernel': 'rbf'}\n",
      "Pipeline(steps=[('columntransformer',\n",
      "                 ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n",
      "                                                              SimpleImputer(strategy='median')),\n",
      "                                                             ('standardscaler',\n",
      "                                                              StandardScaler())]),\n",
      "                                   transformers=[('bedrooms',\n",
      "                                                  Pipeline(steps=[('simpleimputer',\n",
      "                                                                   SimpleImputer(strategy='median')),\n",
      "                                                                  ('functiontransformer',\n",
      "                                                                   FunctionTransformer(feature_names_out=<function ratio_name at 0x000...\n",
      "                                                   'total_rooms', 'population',\n",
      "                                                   'households',\n",
      "                                                   'median_income']),\n",
      "                                                 ('geo', ClusterSimilarity(),\n",
      "                                                  ['latitude', 'longitude']),\n",
      "                                                 ('cat',\n",
      "                                                  Pipeline(steps=[('simpleimputer',\n",
      "                                                                   SimpleImputer(strategy='most_frequent')),\n",
      "                                                                  ('onehotencoder',\n",
      "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
      "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x00000208726A70D0>)])),\n",
      "                ('svr', SVR(C=1000.0, gamma=0.1))])\n",
      "70627.77856744942\n"
     ]
    }
   ],
   "source": [
    "# Grid Search 로 parameter를 찾아보자\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "full_pipeline = make_pipeline(preprocessing, SVR())\n",
    "\n",
    "param_grid = [\n",
    "    {'svr__kernel': ['linear'], 'svr__C': [10., 30., 100., 300., 1000., 3000., 10000., 30000.]},\n",
    "    {'svr__kernel': ['rbf'], 'svr__C': [1.0, 3.0, 10., 30., 100., 300., 1000.], 'svr__gamma': [0.01, 0.03, 0.1, 0.3, 1.0, 3.0]},\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(full_pipeline, param_grid, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(train_set_predictor, train_set_labels)\n",
    "\n",
    "svr_grid_search_rmse = -grid_search.best_score_\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_estimator_)\n",
    "print(svr_grid_search_rmse)  \n",
    "\n",
    "# i9-12900H 에서 대략 6분 정도 걸렸고 그 결과 약 70000 정도의 RMSE를 얻었다. \n",
    "# random forest가 약 40000 정도를 얻은걸 생각하면 별 도움은 안되는 결과인 듯 하다\n",
    "# 그나저나 이건 CPU 파워가 짱이군...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Try replacing the GridSearchCV with a RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71478.61642890856\n",
      "{'svr__C': 1410}\n",
      "Pipeline(steps=[('columntransformer',\n",
      "                 ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n",
      "                                                              SimpleImputer(strategy='median')),\n",
      "                                                             ('standardscaler',\n",
      "                                                              StandardScaler())]),\n",
      "                                   transformers=[('bedrooms',\n",
      "                                                  Pipeline(steps=[('simpleimputer',\n",
      "                                                                   SimpleImputer(strategy='median')),\n",
      "                                                                  ('functiontransformer',\n",
      "                                                                   FunctionTransformer(feature_names_out=<function ratio_name at 0x000...\n",
      "                                                   'households',\n",
      "                                                   'median_income']),\n",
      "                                                 ('geo', ClusterSimilarity(),\n",
      "                                                  ['latitude', 'longitude']),\n",
      "                                                 ('cat',\n",
      "                                                  Pipeline(steps=[('simpleimputer',\n",
      "                                                                   SimpleImputer(strategy='most_frequent')),\n",
      "                                                                  ('onehotencoder',\n",
      "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
      "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x0000021B0B1C5AF0>)])),\n",
      "                ('svr', SVR(C=1410, kernel='linear'))])\n"
     ]
    }
   ],
   "source": [
    "# Randomized Search 로 parameter를 찾아보자\n",
    "import scipy\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "full_pipeline = make_pipeline(preprocessing, SVR(kernel='linear'))\n",
    "\n",
    "param_distribs = {\n",
    "    'svr__C': scipy.stats.randint(low=10, high=30000),\n",
    "}\n",
    "\n",
    "rnd_search = RandomizedSearchCV(\n",
    "    full_pipeline, param_distributions=param_distribs, n_iter=100, cv=3, \n",
    "    scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "rnd_search.fit(train_set_predictor, train_set_labels)\n",
    "svr_rnd_search_rmse = -rnd_search.best_score_\n",
    "\n",
    "print(svr_rnd_search_rmse)\n",
    "print(rnd_search.best_params_)\n",
    "print(rnd_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57995.34706633072\n",
      "{'svr__C': 27866, 'svr__gamma': 0.4922022145550622}\n",
      "Pipeline(steps=[('columntransformer',\n",
      "                 ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n",
      "                                                              SimpleImputer(strategy='median')),\n",
      "                                                             ('standardscaler',\n",
      "                                                              StandardScaler())]),\n",
      "                                   transformers=[('bedrooms',\n",
      "                                                  Pipeline(steps=[('simpleimputer',\n",
      "                                                                   SimpleImputer(strategy='median')),\n",
      "                                                                  ('functiontransformer',\n",
      "                                                                   FunctionTransformer(feature_names_out=<function ratio_name at 0x000...\n",
      "                                                   'households',\n",
      "                                                   'median_income']),\n",
      "                                                 ('geo', ClusterSimilarity(),\n",
      "                                                  ['latitude', 'longitude']),\n",
      "                                                 ('cat',\n",
      "                                                  Pipeline(steps=[('simpleimputer',\n",
      "                                                                   SimpleImputer(strategy='most_frequent')),\n",
      "                                                                  ('onehotencoder',\n",
      "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
      "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x0000021B0B15CFA0>)])),\n",
      "                ('svr', SVR(C=27866, gamma=0.4922022145550622))])\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "\n",
    "# kernel을 rbf로 바꿔서 다시 한번\n",
    "\n",
    "full_pipeline = make_pipeline(preprocessing, SVR(kernel='rbf'))\n",
    "\n",
    "param_distribs = {\n",
    "    'svr__C': scipy.stats.randint(low=10, high=30000),\n",
    "    'svr__gamma' : scipy.stats.uniform(loc=0.01, scale=3.0),\n",
    "}\n",
    "\n",
    "rnd_search = RandomizedSearchCV(\n",
    "    full_pipeline, param_distributions=param_distribs, n_iter=100, cv=3, \n",
    "    scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "rnd_search.fit(train_set_predictor, train_set_labels)\n",
    "svr_rnd_search_rmse = -rnd_search.best_score_\n",
    "\n",
    "print(svr_rnd_search_rmse)\n",
    "print(rnd_search.best_params_)\n",
    "print(rnd_search.best_estimator_)\n",
    "\n",
    "# 이걸로 해보니 RMSE가 57995 정도 나온다\n",
    "# RBF가 Linear에 비해 좀 더 좋은 결과가 나오긴 한다\n",
    "# 그래봐야 random forest에 비해서는 안좋았으니 의미는 없다만..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Try adding a SelectFromModel transformer in the preparation pipeline to select only the most important attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count        3.000000\n",
       "mean     57878.091756\n",
       "std       1229.630564\n",
       "min      57102.196804\n",
       "25%      57169.219583\n",
       "50%      57236.242363\n",
       "75%      58266.039232\n",
       "max      59295.836102\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "selector_pipeline = make_pipeline(\n",
    "    preprocessing,\n",
    "    SelectFromModel(RandomForestRegressor(), threshold=0.005),  # min feature importance\n",
    "    SVR(C=27866, gamma=0.4922022145550622, kernel='rbf'))\n",
    "\n",
    "selector_rmses = -cross_val_score(selector_pipeline, train_set_predictor, train_set_labels, cv=3, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "pandas.Series(selector_rmses).describe()\n",
    "\n",
    "# 근데 돌리고 보니 저 SelectFromModel은 preprocessing stage의 일부로 들어가야 하는거 아닌가...?\n",
    "# 하지만ㄴ 귀찮으니 그냥 이런게 있다는 걸 기억하고 넘어가는 정도로...."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Try creating a custom transformer that trains a k-nearest neighbors regressor (sklearn.neighbors.KNeighborsRegressor) in its fit() method, and outputs the model’s predictions in its transform() method. Then add this feature to the preprocessing pipeline, using latitude and longitude as the inputs to this transformer. This will add a feature in the model that corresponds to the housing median price of the nearest districts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# KNeighborRegressor는 fit하고 predict는 있는데 tranfrom은 없다\n",
    "# 그래서 굳이 이렇게 FeatureFromRegressor를 쓰는 것이다\n",
    "# 아 그래서 transform에서 predictor를 부른거구나\n",
    "# 해답을 보면 아무 regressor나 쓸 수 있게 만든다고 이렇게 했다는데 그게 처음엔 잘 이해가 안됐었는데 이제는 좀 이해가 됨\n",
    "# \n",
    "\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.base import MetaEstimatorMixin, clone\n",
    "\n",
    "class FeatureFromRegressor(MetaEstimatorMixin, BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, estimator):\n",
    "        self.estimator = estimator\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        estimator_ = clone(self.estimator)\n",
    "        estimator_.fit(X, y)\n",
    "        self.estimator_ = estimator_\n",
    "        self.n_features_in_ = self.estimator_.n_features_in_\n",
    "        if hasattr(self.estimator, \"feature_names_in_\"):\n",
    "            self.feature_names_in_ = self.estimator.feature_names_in_\n",
    "        return self  # always return self!\n",
    "    \n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self)\n",
    "        predictions = self.estimator_.predict(X)\n",
    "        if predictions.ndim == 1:\n",
    "            predictions = predictions.reshape(-1, 1)\n",
    "        return predictions\n",
    "\n",
    "    def get_feature_names_out(self, names=None):\n",
    "        check_is_fitted(self)\n",
    "        n_outputs = getattr(self.estimator_, \"n_outputs_\", 1)\n",
    "        estimator_class_name = self.estimator_.__class__.__name__\n",
    "        estimator_short_name = estimator_class_name.lower().replace(\"_\", \"\")\n",
    "        return [f\"{estimator_short_name}_prediction_{i}\"\n",
    "                for i in range(n_outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[486100.66666667],\n",
       "       [435250.        ],\n",
       "       [105100.        ],\n",
       "       ...,\n",
       "       [148800.        ],\n",
       "       [500001.        ],\n",
       "       [234333.33333333]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "\n",
    "check_estimator(FeatureFromRegressor(KNeighborsRegressor()))\n",
    "knn_reg = KNeighborsRegressor(n_neighbors=3, weights=\"distance\")\n",
    "knn_transformer = FeatureFromRegressor(knn_reg)\n",
    "geo_features = train_set_predictor[[\"latitude\", \"longitude\"]]\n",
    "knn_transformer.fit_transform(geo_features, train_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kneighborsregressor_prediction_0']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_transformer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count         3.000000\n",
       "mean     104215.465769\n",
       "std         515.524175\n",
       "min      103664.301464\n",
       "25%      103980.298351\n",
       "50%      104296.295238\n",
       "75%      104491.047921\n",
       "max      104685.800604\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "transformers = [(name, clone(transformer), columns)\n",
    "                for name, transformer, columns in preprocessing.transformers]\n",
    "geo_index = [name for name, _, _ in transformers].index(\"geo\")\n",
    "transformers[geo_index] = (\"geo\", knn_transformer, [\"latitude\", \"longitude\"])\n",
    "\n",
    "# KneighborsRegressor는 fit하고 predict밖에 업서여...\n",
    "# transformers[geo_index] = (\"geo\", KNeighborsRegressor(n_neighbors=3, weights=\"distance\"), [\"latitude\", \"longitude\"])\n",
    "\n",
    "new_geo_preprocessing = ColumnTransformer(transformers)\n",
    "new_geo_pipeline = Pipeline([\n",
    "    ('preprocessing', new_geo_preprocessing),\n",
    "    ('svr', SVR(C=27866, gamma=0.4922022145550622, kernel='rbf')),\n",
    "])\n",
    "\n",
    "new_pipe_rmses = -cross_val_score(new_geo_pipeline, train_set_predictor, train_set_labels, scoring=\"neg_root_mean_squared_error\", cv=3)\n",
    "pandas.Series(new_pipe_rmses).describe()\n",
    "\n",
    "# 생각보다 RMSE는 안나옴. 100,000이나 나오네\n",
    "# 근데 당연하다면 당연하다. 이럴거면 parameter도 다시 맞춰야지"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Automatically explore some preparation options using ~~GridSearchCV~~RandomSearchCV.\n",
    "- RandomSearchCV가 GridSearchCV보다 하기 좋아서 이걸로 함\n",
    "- 해답도 이걸로 되어 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88847.05880192488\n",
      "{'preprocessing__geo__estimator__n_neighbors': 1, 'preprocessing__geo__estimator__weights': 'distance', 'svr__C': 84510.15507675285, 'svr__gamma': 0.816499237057516}\n",
      "Pipeline(steps=[('preprocessing',\n",
      "                 ColumnTransformer(transformers=[('bedrooms',\n",
      "                                                  Pipeline(steps=[('simpleimputer',\n",
      "                                                                   SimpleImputer(strategy='median')),\n",
      "                                                                  ('functiontransformer',\n",
      "                                                                   FunctionTransformer(feature_names_out=<function ratio_name at 0x0000021B08E87040>,\n",
      "                                                                                       func=<function column_ratio at 0x0000021B052428B0>)),\n",
      "                                                                  ('standardscaler',\n",
      "                                                                   StandardScaler())]),\n",
      "                                                  ['total_bedrooms...\n",
      "                                                  FeatureFromRegressor(estimator=KNeighborsRegressor(n_neighbors=1,\n",
      "                                                                                                     weights='distance')),\n",
      "                                                  ['latitude', 'longitude']),\n",
      "                                                 ('cat',\n",
      "                                                  Pipeline(steps=[('simpleimputer',\n",
      "                                                                   SimpleImputer(strategy='most_frequent')),\n",
      "                                                                  ('onehotencoder',\n",
      "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
      "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x0000021B1B824DC0>)])),\n",
      "                ('svr', SVR(C=84510.15507675285, gamma=0.816499237057516))])\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import expon, loguniform\n",
    "\n",
    "param_distribs = {\n",
    "    \"preprocessing__geo__estimator__n_neighbors\": range(1, 30),\n",
    "    \"preprocessing__geo__estimator__weights\": [\"distance\", \"uniform\"],\n",
    "    \"svr__C\": loguniform(20, 200_000),\n",
    "    \"svr__gamma\": expon(scale=1.0),\n",
    "}\n",
    "\n",
    "new_geo_rnd_search = RandomizedSearchCV(new_geo_pipeline,\n",
    "                                        param_distributions=param_distribs,\n",
    "                                        n_iter=50,\n",
    "                                        cv=3,\n",
    "                                        scoring='neg_root_mean_squared_error',\n",
    "                                        n_jobs=-1)\n",
    "new_geo_rnd_search.fit(train_set_predictor, train_set_labels)\n",
    "new_geo_rnd_search_rmse = -new_geo_rnd_search.best_score_\n",
    "\n",
    "print(new_geo_rnd_search_rmse)\n",
    "print(new_geo_rnd_search.best_params_)\n",
    "print(new_geo_rnd_search.best_estimator_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Exercise: Try to implement the StandardScalerClone class again from scratch, then add support for the inverse_transform() method: executing scaler.inverse_transform(scaler.fit_transform(X)) should return an array very close to X. Then add support for feature names: set feature_names_in_ in the fit() method if the input is a DataFrame. This attribute should be a NumPy array of column names. Lastly, implement the get_feature_names_out() method: it should have one optional input_features=None argument. If passed, the method should check that its length matches n_features_in_, and it should match feature_names_in_ if it is defined, then input_features should be returned. If input_features is None, then the method should return feature_names_in_ if it is defined or np.array([\"x0\", \"x1\", ...]) with length n_features_in_ otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_array, check_is_fitted\n",
    "\n",
    "class StandardScalerClone(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, with_mean=True):  # no *args or **kwargs!\n",
    "        self.with_mean = with_mean\n",
    "\n",
    "    def fit(self, X, y=None):  # y is required even though we don't use it\n",
    "        X_orig = X\n",
    "        X = check_array(X)  # checks that X is an array with finite float values\n",
    "        self.mean_ = X.mean(axis=0)\n",
    "        self.scale_ = X.std(axis=0)\n",
    "        self.n_features_in_ = X.shape[1]  # every estimator stores this in fit()\n",
    "        if hasattr(X_orig, \"columns\"):\n",
    "            self.feature_names_in_ = numpy.array(X_orig.columns, dtype=object)\n",
    "        return self  # always return self!\n",
    "\n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self)  # looks for learned attributes (with trailing _)\n",
    "        X = check_array(X)\n",
    "        if self.n_features_in_ != X.shape[1]:\n",
    "            raise ValueError(\"Unexpected number of features\")\n",
    "        if self.with_mean:\n",
    "            X = X - self.mean_\n",
    "        return X / self.scale_\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        check_is_fitted(self)\n",
    "        X = check_array(X)\n",
    "        if self.n_features_in_ != X.shape[1]:\n",
    "            raise ValueError(\"Unexpected number of features\")\n",
    "        X = X * self.scale_\n",
    "        return X + self.mean_ if self.with_mean else X\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        if input_features is None:\n",
    "            return getattr(self, \"feature_names_in_\",\n",
    "                           [f\"x{i}\" for i in range(self.n_features_in_)])\n",
    "        else:\n",
    "            if len(input_features) != self.n_features_in_:\n",
    "                raise ValueError(\"Invalid number of features\")\n",
    "            if hasattr(self, \"feature_names_in_\") and not numpy.all(\n",
    "                self.feature_names_in_ == input_features\n",
    "            ):\n",
    "                raise ValueError(\"input_features ≠ feature_names_in_\")\n",
    "            return input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test our custom estimator\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    " \n",
    "check_estimator(StandardScalerClone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure it works\n",
    "\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.allclose.html\n",
    "# numpy.allclose(a, b, rtol=1e-05, atol=1e-08, equal_nan=False)[source]\n",
    "# Returns True if two arrays are element-wise equal within a tolerance.\n",
    "# absolute(a - b) <= (atol + rtol * absolute(b))\n",
    "\n",
    "X = numpy.random.rand(1000, 3)\n",
    "\n",
    "scaler = StandardScalerClone()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "assert numpy.allclose(X_scaled, (X - X.mean(axis=0)) / X.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when with_mean = False\n",
    "\n",
    "scaler = StandardScalerClone(with_mean=False)\n",
    "X_scaled_uncentered = scaler.fit_transform(X)\n",
    "\n",
    "assert numpy.allclose(X_scaled_uncentered, X / X.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse work?\n",
    "\n",
    "scaler = StandardScalerClone()\n",
    "X_back = scaler.inverse_transform(scaler.fit_transform(X))\n",
    "\n",
    "assert numpy.allclose(X, X_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How about the feature names out?\n",
    "\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.all.html\n",
    "# numpy.all : Test whether all array elements along a given axis evaluate to True.\n",
    "\n",
    "\n",
    "assert numpy.all(scaler.get_feature_names_out() == [\"x0\", \"x1\", \"x2\"])\n",
    "assert numpy.all(scaler.get_feature_names_out([\"a\", \"b\", \"c\"]) == [\"a\", \"b\", \"c\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7352\\1033146809.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# And if we fit a DataFrame, are the feature in and out ok?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"a\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"b\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScalerClone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# And if we fit a DataFrame, are the feature in and out ok?\n",
    "\n",
    "df = pandas.DataFrame({\"a\": numpy.random.rand(100), \"b\": numpy.random.rand(100)})\n",
    "scaler = StandardScalerClone()\n",
    "X_scaled = scaler.fit_transform(df)\n",
    "\n",
    "assert numpy.all(scaler.feature_names_in_ == [\"a\", \"b\"])\n",
    "assert numpy.all(scaler.get_feature_names_out() == [\"a\", \"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
